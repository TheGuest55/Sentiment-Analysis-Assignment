{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">\n",
    "    <tr>\n",
    "        <th style=\"text-align:center; width:25%\"><img src='https://www.np.edu.sg/PublishingImages/Pages/default/odp/ICT.jpg' style=\"width: 250px; height: 125px; \"></th>\n",
    "        <th style=\"text-align:center;\"><h1>Deep Learning</h1><h2>Assignment 2 - Character Generator Model (Problem 2)</h2><h3>AY2020/21 Semester</h3></th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 & 1: Import Packages & Data Loading, processing and sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras:  2.3.0-tf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original text has 562439 characters.\n",
      "\n",
      "This document has 544340 total number of characters.\n",
      "This document has 35 unique characters.\n",
      "['the adventures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-h', 'he adventures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-he', 'e adventures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-hea', ' adventures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-head', 'adventures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-heade', 'dventures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-headed', 'ventures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-headed ', 'entures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-headed l', 'ntures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-headed le', 'tures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-headed lea']\n",
      "['e', 'a', 'd', 'e', 'd', ' ', 'l', 'e', 'a', 'g']\n",
      "\n",
      "Number of sequences: 544240\n",
      "Vectorization...(One-hot encoding)\n"
     ]
    }
   ],
   "source": [
    "# Import the Required Packages\n",
    "from tensorflow import keras\n",
    "print('keras: ', keras.__version__)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Flatten, Dense, GRU, LSTM\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "import numpy as np\n",
    "\n",
    "#================================================================================\n",
    "\n",
    "# read in the text file, transforming everything to lower case\n",
    "text = open('holmes - Copy.txt').read().lower()\n",
    "print('The original text has ' + str(len(text)) + ' characters.\\n')\n",
    "\n",
    "# remove all '\\n' and '\\r' from text\n",
    "text = text.replace('\\n','') \n",
    "text = text.replace('\\r','')\n",
    "\n",
    "# create a function 'clean_text' to clean text so that only the following letters and punctation remain\n",
    "def clean_text(text):\n",
    "    punctuation = ['!', ',', '.', ':', ';', '?', '-', \"'\",' ']\n",
    "    letters='abcdefghijklmnopqrstuvwxyz'\n",
    "    cText =[]\n",
    "    # Enter your code here:\n",
    "    for i in text:\n",
    "        for j in letters:\n",
    "            if i == j:\n",
    "                cText.append(i)\n",
    "                \n",
    "                \n",
    "        for k in punctuation:\n",
    "            if i == k:\n",
    "                cText.append(i)\n",
    "                \n",
    "    cText_string = ''.join(cText)\n",
    "    return cText_string\n",
    "\n",
    "# clean data using clean_text function\n",
    "text = clean_text(text)\n",
    "\n",
    "\n",
    "# count the number of unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# print some of the text, as well as statistics\n",
    "print (\"This document has \" +  str(len(text)) + \" total number of characters.\")\n",
    "print (\"This document has \" +  str(len(chars)) + \" unique characters.\")\n",
    "\n",
    "# create a function 'generate_text_io' to generate text inputs\n",
    "# based on window_size and the corresponding labels\n",
    "def generate_text_io(text, window_size):\n",
    "    inputs = [] # store inputs\n",
    "    labels = [] # stores label\n",
    "    \n",
    "    # Enter your code here:\n",
    "    for i in range(0, len(text) - window_size):\n",
    "        inputs.append(text[i: i + window_size])\n",
    "        labels.append(text[i + window_size])\n",
    "    print(inputs[0:10])\n",
    "    print(labels[0:10])\n",
    "    print()\n",
    "    x=print('Number of sequences:', len(inputs))\n",
    "    return inputs, labels\n",
    "\n",
    "# this dictionary is a function mapping each unique character to a unique integer\n",
    "chars_to_indices = dict((c, i) for i, c in enumerate(chars))  # map each unique character to unique integer\n",
    "\n",
    "# this dictionary is a function mapping each unique integer back to a unique character\n",
    "indices_to_chars = dict((i, c) for i, c in enumerate(chars))  # map each unique integer back to unique character\n",
    "\n",
    "# create a function 'encode_io_pairs' to perform one-hot encoding of inputs and labels\n",
    "def encode_io_pairs(text,window_size): # window_size determines # of characters in each input\n",
    "    inputs, labels= generate_text_io(text, window_size)\n",
    "\n",
    "    # Enter your code here:\n",
    "    print('Vectorization...(One-hot encoding)')\n",
    "    x = np.zeros((len(inputs), window_size, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(inputs), len(chars)), dtype=np.bool)\n",
    "    \n",
    "    for i, sentence in enumerate(inputs):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, chars_to_indices[char]] = 1\n",
    "        y[i,chars_to_indices[labels[i]]] = 1\n",
    "    return x,y\n",
    "\n",
    "# perform one-hot encoding of inputs and labels\n",
    "window_size = 100\n",
    "X, y = encode_io_pairs(text, window_size)\n",
    "\n",
    "#================================================================================\n",
    "    \n",
    "# Split the X & y into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 7)\n",
    "# Refer the report Appendix\n",
    "# Please enter the random_state assigned to your group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 â€“ Develop a Character Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 (Replicate as necessary for other models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, 256)               74752     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 35)                3535      \n",
      "=================================================================\n",
      "Total params: 103,987\n",
      "Trainable params: 103,987\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the Model\n",
    "# Enter your code here:\n",
    "max_features = 50000\n",
    "max_len = 100\n",
    "model = Sequential()\n",
    "#model.add(Embedding(max_features,35,input_length=max_len))\n",
    "model.add(SimpleRNN(256, input_shape=(window_size, len(chars))))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   2/3402 [..............................] - ETA: 7:29 - loss: 3.5903 - acc: 0.0273WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.101185). Check your callbacks.\n",
      "3402/3402 [==============================] - 251s 74ms/step - loss: 2.4943 - acc: 0.2967\n",
      "Epoch 2/20\n",
      "3402/3402 [==============================] - 252s 74ms/step - loss: 2.2041 - acc: 0.3674\n",
      "Epoch 3/20\n",
      "3402/3402 [==============================] - 250s 73ms/step - loss: 2.0974 - acc: 0.3947\n",
      "Epoch 4/20\n",
      "3402/3402 [==============================] - 242s 71ms/step - loss: 2.0233 - acc: 0.4128\n",
      "Epoch 5/20\n",
      "3402/3402 [==============================] - 243s 72ms/step - loss: 1.9674 - acc: 0.4277\n",
      "Epoch 6/20\n",
      "3402/3402 [==============================] - 244s 72ms/step - loss: 1.9227 - acc: 0.4396\n",
      "Epoch 7/20\n",
      "3402/3402 [==============================] - 242s 71ms/step - loss: 1.8863 - acc: 0.4492\n",
      "Epoch 8/20\n",
      "3402/3402 [==============================] - 240s 70ms/step - loss: 1.8557 - acc: 0.4584\n",
      "Epoch 9/20\n",
      "3402/3402 [==============================] - 250s 73ms/step - loss: 1.8297 - acc: 0.4656\n",
      "Epoch 10/20\n",
      "3402/3402 [==============================] - 263s 77ms/step - loss: 1.8063 - acc: 0.4720\n",
      "Epoch 11/20\n",
      "3402/3402 [==============================] - 270s 79ms/step - loss: 1.7850 - acc: 0.4787\n",
      "Epoch 12/20\n",
      "3402/3402 [==============================] - 244s 72ms/step - loss: 1.7658 - acc: 0.4835\n",
      "Epoch 13/20\n",
      "3402/3402 [==============================] - 237s 70ms/step - loss: 1.7478 - acc: 0.4888\n",
      "Epoch 14/20\n",
      "3402/3402 [==============================] - 243s 72ms/step - loss: 1.7316 - acc: 0.4936\n",
      "Epoch 15/20\n",
      "3402/3402 [==============================] - 242s 71ms/step - loss: 1.7167 - acc: 0.4981\n",
      "Epoch 16/20\n",
      "3402/3402 [==============================] - 238s 70ms/step - loss: 1.7023 - acc: 0.5020\n",
      "Epoch 17/20\n",
      "3402/3402 [==============================] - 233s 68ms/step - loss: 1.6886 - acc: 0.5067\n",
      "Epoch 18/20\n",
      "3402/3402 [==============================] - 239s 70ms/step - loss: 1.6761 - acc: 0.5098\n",
      "Epoch 19/20\n",
      "3402/3402 [==============================] - 242s 71ms/step - loss: 1.6644 - acc: 0.5129\n",
      "Epoch 20/20\n",
      "3402/3402 [==============================] - 239s 70ms/step - loss: 1.6534 - acc: 0.5164\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "# Enter your code here:\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir='p2_log_dir_m1',histogram_freq=1,)]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4),metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "#model.save('chgen_model_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_5 (GRU)                  (None, 128)               63360     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 35)                1155      \n",
      "=================================================================\n",
      "Total params: 68,643\n",
      "Trainable params: 68,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features = 50000\n",
    "max_len = 100\n",
    "model = Sequential()\n",
    "model.add(GRU(128,input_shape=(window_size, len(chars))))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.9367 - acc: 0.4338\n",
      "Epoch 2/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.9103 - acc: 0.4412\n",
      "Epoch 3/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.8863 - acc: 0.4481\n",
      "Epoch 4/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.8647 - acc: 0.4536\n",
      "Epoch 5/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.8449 - acc: 0.4595\n",
      "Epoch 6/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.8268 - acc: 0.4645\n",
      "Epoch 7/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.8097 - acc: 0.4697\n",
      "Epoch 8/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7938 - acc: 0.4743\n",
      "Epoch 9/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7789 - acc: 0.4786\n",
      "Epoch 10/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7649 - acc: 0.4825\n",
      "Epoch 11/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7516 - acc: 0.4862\n",
      "Epoch 12/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7390 - acc: 0.4895\n",
      "Epoch 13/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7272 - acc: 0.4933\n",
      "Epoch 14/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7160 - acc: 0.4963\n",
      "Epoch 15/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7052 - acc: 0.4998\n",
      "Epoch 16/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6948 - acc: 0.5030\n",
      "Epoch 17/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6850 - acc: 0.5057\n",
      "Epoch 18/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6756 - acc: 0.5082\n",
      "Epoch 19/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6667 - acc: 0.5110\n",
      "Epoch 20/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6581 - acc: 0.5140\n",
      "Epoch 21/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6498 - acc: 0.5163\n",
      "Epoch 22/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6419 - acc: 0.5192\n",
      "Epoch 23/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6343 - acc: 0.5208\n",
      "Epoch 24/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6269 - acc: 0.5235\n",
      "Epoch 25/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6199 - acc: 0.5258\n",
      "Epoch 26/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6132 - acc: 0.5274\n",
      "Epoch 27/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6066 - acc: 0.5293\n",
      "Epoch 28/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6003 - acc: 0.5315\n",
      "Epoch 29/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5943 - acc: 0.5332\n",
      "Epoch 30/30\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5883 - acc: 0.5349\n"
     ]
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.TensorBoard(log_dir='p2_log_dir_m2',histogram_freq=1,)]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4),metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=128,\n",
    "                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 2.6915 - acc: 0.2485\n",
      "Epoch 2/50\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 2.3556 - acc: 0.3295\n",
      "Epoch 3/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 2.2366 - acc: 0.3550\n",
      "Epoch 4/50\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 2.1518 - acc: 0.3707\n",
      "Epoch 5/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 2.0869 - acc: 0.3892\n",
      "Epoch 6/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 2.0344 - acc: 0.4060\n",
      "Epoch 7/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.9918 - acc: 0.4178\n",
      "Epoch 8/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.9573 - acc: 0.4292\n",
      "Epoch 9/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.9279 - acc: 0.4382\n",
      "Epoch 10/50\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.9023 - acc: 0.4456\n",
      "Epoch 11/50\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.8798 - acc: 0.4517\n",
      "Epoch 12/50\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.8599 - acc: 0.4570\n",
      "Epoch 13/50\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.8414 - acc: 0.4617\n",
      "Epoch 14/50\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.8243 - acc: 0.4666\n",
      "Epoch 15/50\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.8086 - acc: 0.4710\n",
      "Epoch 16/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7938 - acc: 0.4754\n",
      "Epoch 17/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7800 - acc: 0.4792\n",
      "Epoch 18/50\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.7667 - acc: 0.4835\n",
      "Epoch 19/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7542 - acc: 0.4873\n",
      "Epoch 20/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7425 - acc: 0.4906\n",
      "Epoch 21/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7311 - acc: 0.4940\n",
      "Epoch 22/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7203 - acc: 0.4972\n",
      "Epoch 23/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.7099 - acc: 0.5006\n",
      "Epoch 24/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6999 - acc: 0.5031\n",
      "Epoch 25/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6904 - acc: 0.5057\n",
      "Epoch 26/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6814 - acc: 0.5079\n",
      "Epoch 27/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6728 - acc: 0.5107\n",
      "Epoch 28/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6643 - acc: 0.5133\n",
      "Epoch 29/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6563 - acc: 0.5152\n",
      "Epoch 30/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6485 - acc: 0.5179\n",
      "Epoch 31/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6411 - acc: 0.5197\n",
      "Epoch 32/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6341 - acc: 0.5215\n",
      "Epoch 33/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6272 - acc: 0.5232\n",
      "Epoch 34/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6205 - acc: 0.5253\n",
      "Epoch 35/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6142 - acc: 0.5269\n",
      "Epoch 36/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6081 - acc: 0.5293\n",
      "Epoch 37/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.6023 - acc: 0.5304\n",
      "Epoch 38/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5965 - acc: 0.5320\n",
      "Epoch 39/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5911 - acc: 0.5336\n",
      "Epoch 40/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5857 - acc: 0.5353\n",
      "Epoch 41/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5808 - acc: 0.5368\n",
      "Epoch 42/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5756 - acc: 0.5383\n",
      "Epoch 43/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5707 - acc: 0.5396\n",
      "Epoch 44/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5660 - acc: 0.5407\n",
      "Epoch 45/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5614 - acc: 0.5423\n",
      "Epoch 46/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5572 - acc: 0.5433\n",
      "Epoch 47/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5528 - acc: 0.5445\n",
      "Epoch 48/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5486 - acc: 0.5459\n",
      "Epoch 49/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5444 - acc: 0.5468\n",
      "Epoch 50/50\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5407 - acc: 0.5478\n"
     ]
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.TensorBoard(log_dir='p2_log_dir_m2',histogram_freq=1,)]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4),metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=128,\n",
    "                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5369 - acc: 0.5491\n",
      "Epoch 2/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5330 - acc: 0.5503\n",
      "Epoch 3/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5292 - acc: 0.5519\n",
      "Epoch 4/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5256 - acc: 0.5521\n",
      "Epoch 5/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5223 - acc: 0.5534\n",
      "Epoch 6/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5188 - acc: 0.5542\n",
      "Epoch 7/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.5156 - acc: 0.5552\n",
      "Epoch 8/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.5122 - acc: 0.5564\n",
      "Epoch 9/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.5091 - acc: 0.5565\n",
      "Epoch 10/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.5060 - acc: 0.5581\n",
      "Epoch 11/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.5029 - acc: 0.5590\n",
      "Epoch 12/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.5000 - acc: 0.5596\n",
      "Epoch 13/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.4970 - acc: 0.5605\n",
      "Epoch 14/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.4941 - acc: 0.5609\n",
      "Epoch 15/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.4916 - acc: 0.5620\n",
      "Epoch 16/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.4888 - acc: 0.5626\n",
      "Epoch 17/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.4860 - acc: 0.5637\n",
      "Epoch 18/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.4835 - acc: 0.5639\n",
      "Epoch 19/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.4809 - acc: 0.5651\n",
      "Epoch 20/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4785 - acc: 0.5655\n",
      "Epoch 21/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4760 - acc: 0.5663\n",
      "Epoch 22/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4737 - acc: 0.5670\n",
      "Epoch 23/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.4711 - acc: 0.5681\n",
      "Epoch 24/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.4689 - acc: 0.5686\n",
      "Epoch 25/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.4667 - acc: 0.5692\n",
      "Epoch 26/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4644 - acc: 0.5696\n",
      "Epoch 27/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.4620 - acc: 0.5699\n",
      "Epoch 28/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4598 - acc: 0.5707\n",
      "Epoch 29/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4581 - acc: 0.5714\n",
      "Epoch 30/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4559 - acc: 0.5719\n",
      "Epoch 31/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4538 - acc: 0.5724\n",
      "Epoch 32/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4518 - acc: 0.5732\n",
      "Epoch 33/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4497 - acc: 0.5736\n",
      "Epoch 34/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4477 - acc: 0.5740\n",
      "Epoch 35/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4459 - acc: 0.5748\n",
      "Epoch 36/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4441 - acc: 0.5754\n",
      "Epoch 37/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4424 - acc: 0.5754\n",
      "Epoch 38/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4404 - acc: 0.5760\n",
      "Epoch 39/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4385 - acc: 0.5767\n",
      "Epoch 40/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4366 - acc: 0.5773\n",
      "Epoch 41/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4351 - acc: 0.5773\n",
      "Epoch 42/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4334 - acc: 0.5783\n",
      "Epoch 43/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4317 - acc: 0.5787\n",
      "Epoch 44/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4298 - acc: 0.5789\n",
      "Epoch 45/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4284 - acc: 0.5799\n",
      "Epoch 46/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4267 - acc: 0.5799\n",
      "Epoch 47/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4251 - acc: 0.5803\n",
      "Epoch 48/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4235 - acc: 0.5806\n",
      "Epoch 49/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4220 - acc: 0.5814\n",
      "Epoch 50/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4205 - acc: 0.5817\n",
      "Epoch 51/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4190 - acc: 0.5819\n",
      "Epoch 52/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4175 - acc: 0.5829\n",
      "Epoch 53/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4161 - acc: 0.5832\n",
      "Epoch 54/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4147 - acc: 0.5837\n",
      "Epoch 55/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4135 - acc: 0.5838\n",
      "Epoch 56/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4118 - acc: 0.5845\n",
      "Epoch 57/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4106 - acc: 0.5846\n",
      "Epoch 58/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4092 - acc: 0.5851\n",
      "Epoch 59/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4081 - acc: 0.5850\n",
      "Epoch 60/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4067 - acc: 0.5852\n",
      "Epoch 61/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4055 - acc: 0.5860\n",
      "Epoch 62/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4041 - acc: 0.5862\n",
      "Epoch 63/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4029 - acc: 0.5861\n",
      "Epoch 64/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4017 - acc: 0.5866\n",
      "Epoch 65/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.4006 - acc: 0.5875\n",
      "Epoch 66/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3993 - acc: 0.5876\n",
      "Epoch 67/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3981 - acc: 0.5876\n",
      "Epoch 68/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3969 - acc: 0.5885\n",
      "Epoch 69/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3958 - acc: 0.5884\n",
      "Epoch 70/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3947 - acc: 0.5889\n",
      "Epoch 71/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3935 - acc: 0.5890\n",
      "Epoch 72/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3923 - acc: 0.5893\n",
      "Epoch 73/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3914 - acc: 0.5896\n",
      "Epoch 74/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3903 - acc: 0.5898\n",
      "Epoch 75/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3892 - acc: 0.5901\n",
      "Epoch 76/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3883 - acc: 0.5904\n",
      "Epoch 77/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3871 - acc: 0.5907\n",
      "Epoch 78/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3861 - acc: 0.5911\n",
      "Epoch 79/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3851 - acc: 0.5913\n",
      "Epoch 80/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3840 - acc: 0.5914\n",
      "Epoch 81/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3831 - acc: 0.5916\n",
      "Epoch 82/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3821 - acc: 0.5922\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3812 - acc: 0.5921\n",
      "Epoch 84/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3803 - acc: 0.5925\n",
      "Epoch 85/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3793 - acc: 0.5927\n",
      "Epoch 86/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3784 - acc: 0.5927\n",
      "Epoch 87/100\n",
      "3402/3402 [==============================] - 32s 10ms/step - loss: 1.3775 - acc: 0.5931\n",
      "Epoch 88/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3767 - acc: 0.5935\n",
      "Epoch 89/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3754 - acc: 0.5934\n",
      "Epoch 90/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3746 - acc: 0.5935\n",
      "Epoch 91/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3739 - acc: 0.5943\n",
      "Epoch 92/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3730 - acc: 0.5942\n",
      "Epoch 93/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3721 - acc: 0.5940\n",
      "Epoch 94/100\n",
      "3402/3402 [==============================] - 30s 9ms/step - loss: 1.3713 - acc: 0.5950\n",
      "Epoch 95/100\n",
      "3402/3402 [==============================] - 31s 9ms/step - loss: 1.3707 - acc: 0.5949\n",
      "Epoch 96/100\n",
      "3402/3402 [==============================] - 32s 10ms/step - loss: 1.3696 - acc: 0.5952\n",
      "Epoch 97/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.3689 - acc: 0.5956\n",
      "Epoch 98/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.3682 - acc: 0.5956\n",
      "Epoch 99/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.3674 - acc: 0.5959\n",
      "Epoch 100/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.3665 - acc: 0.5958\n"
     ]
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.TensorBoard(log_dir='p2_log_dir_m2',histogram_freq=1,)]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4),metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=128,\n",
    "                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('chgen_model_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 128)               83968     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 35)                3535      \n",
      "=================================================================\n",
      "Total params: 100,403\n",
      "Trainable params: 100,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features = 50000\n",
    "max_len = 100\n",
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape=(window_size, len(chars))))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 2.7615 - acc: 0.2232\n",
      "Epoch 2/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 2.3903 - acc: 0.3076\n",
      "Epoch 3/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 2.2777 - acc: 0.3416 1s - loss: 2.2811 - acc: 0.3 - ETA: \n",
      "Epoch 4/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 2.1946 - acc: 0.3625\n",
      "Epoch 5/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 2.1304 - acc: 0.3785\n",
      "Epoch 6/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 2.0792 - acc: 0.3937\n",
      "Epoch 7/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 2.0365 - acc: 0.4054\n",
      "Epoch 8/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.9991 - acc: 0.4153\n",
      "Epoch 9/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.9664 - acc: 0.4244\n",
      "Epoch 10/100\n",
      "3402/3402 [==============================] - 36s 11ms/step - loss: 1.9377 - acc: 0.4326\n",
      "Epoch 11/100\n",
      "3402/3402 [==============================] - 36s 10ms/step - loss: 1.9112 - acc: 0.4399 1s - loss: 1\n",
      "Epoch 12/100\n",
      "3402/3402 [==============================] - 36s 11ms/step - loss: 1.8873 - acc: 0.4464\n",
      "Epoch 13/100\n",
      "3402/3402 [==============================] - 36s 11ms/step - loss: 1.8657 - acc: 0.4525\n",
      "Epoch 14/100\n",
      "3402/3402 [==============================] - 36s 11ms/step - loss: 1.8455 - acc: 0.4574\n",
      "Epoch 15/100\n",
      "3402/3402 [==============================] - 36s 10ms/step - loss: 1.8267 - acc: 0.4626\n",
      "Epoch 16/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.8096 - acc: 0.4678\n",
      "Epoch 17/100\n",
      "3402/3402 [==============================] - 37s 11ms/step - loss: 1.7934 - acc: 0.4720\n",
      "Epoch 18/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.7784 - acc: 0.4763\n",
      "Epoch 19/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.7642 - acc: 0.4802\n",
      "Epoch 20/100\n",
      "3402/3402 [==============================] - 38s 11ms/step - loss: 1.7506 - acc: 0.4839\n",
      "Epoch 21/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.7377 - acc: 0.4875\n",
      "Epoch 22/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.7257 - acc: 0.4910\n",
      "Epoch 23/100\n",
      "3402/3402 [==============================] - ETA: 0s - loss: 1.7141 - acc: 0.494 - 35s 10ms/step - loss: 1.7141 - acc: 0.4947\n",
      "Epoch 24/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.7030 - acc: 0.4974\n",
      "Epoch 25/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.6921 - acc: 0.5004\n",
      "Epoch 26/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.6818 - acc: 0.5039\n",
      "Epoch 27/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.6722 - acc: 0.5059\n",
      "Epoch 28/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.6627 - acc: 0.5089\n",
      "Epoch 29/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.6540 - acc: 0.5122\n",
      "Epoch 30/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.6453 - acc: 0.5142\n",
      "Epoch 31/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.6371 - acc: 0.5162\n",
      "Epoch 32/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.6294 - acc: 0.5190\n",
      "Epoch 33/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.6218 - acc: 0.5213\n",
      "Epoch 34/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.6145 - acc: 0.5233\n",
      "Epoch 35/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.6073 - acc: 0.5257\n",
      "Epoch 36/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.6005 - acc: 0.5280\n",
      "Epoch 37/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.5941 - acc: 0.5292\n",
      "Epoch 38/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.5878 - acc: 0.5317\n",
      "Epoch 39/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.5814 - acc: 0.5332\n",
      "Epoch 40/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.5757 - acc: 0.5348\n",
      "Epoch 41/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.5697 - acc: 0.5368 0s - loss: 1.5698\n",
      "Epoch 42/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.5646 - acc: 0.5382\n",
      "Epoch 43/100\n",
      "3402/3402 [==============================] - 36s 11ms/step - loss: 1.5589 - acc: 0.5397\n",
      "Epoch 44/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.5536 - acc: 0.5417\n",
      "Epoch 45/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.5488 - acc: 0.5429 0s - loss: 1.5490 - acc:\n",
      "Epoch 46/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.5438 - acc: 0.5445\n",
      "Epoch 47/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.5391 - acc: 0.5456\n",
      "Epoch 48/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.5342 - acc: 0.5467\n",
      "Epoch 49/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.5298 - acc: 0.5486\n",
      "Epoch 50/100\n",
      "3402/3402 [==============================] - 37s 11ms/step - loss: 1.5257 - acc: 0.5497\n",
      "Epoch 51/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.5211 - acc: 0.5508\n",
      "Epoch 52/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.5174 - acc: 0.5518\n",
      "Epoch 53/100\n",
      "3402/3402 [==============================] - 36s 11ms/step - loss: 1.5134 - acc: 0.5531\n",
      "Epoch 54/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.5096 - acc: 0.5537\n",
      "Epoch 55/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.5058 - acc: 0.5549\n",
      "Epoch 56/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.5025 - acc: 0.5558\n",
      "Epoch 57/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.4987 - acc: 0.5572\n",
      "Epoch 58/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.4953 - acc: 0.5575\n",
      "Epoch 59/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.4917 - acc: 0.5587\n",
      "Epoch 60/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.4884 - acc: 0.5593\n",
      "Epoch 61/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.4853 - acc: 0.5605\n",
      "Epoch 62/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.4821 - acc: 0.5611\n",
      "Epoch 63/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.4792 - acc: 0.5625\n",
      "Epoch 64/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.4763 - acc: 0.5630\n",
      "Epoch 65/100\n",
      "3402/3402 [==============================] - 33s 10ms/step - loss: 1.4732 - acc: 0.5638\n",
      "Epoch 66/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4707 - acc: 0.5647\n",
      "Epoch 67/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4676 - acc: 0.5655\n",
      "Epoch 68/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.4650 - acc: 0.5659\n",
      "Epoch 69/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.4619 - acc: 0.5671\n",
      "Epoch 70/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4595 - acc: 0.5680\n",
      "Epoch 71/100\n",
      "3402/3402 [==============================] - 36s 11ms/step - loss: 1.4570 - acc: 0.5687\n",
      "Epoch 72/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4546 - acc: 0.5690\n",
      "Epoch 73/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.4520 - acc: 0.5691\n",
      "Epoch 74/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.4494 - acc: 0.5701\n",
      "Epoch 75/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4472 - acc: 0.5713\n",
      "Epoch 76/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4449 - acc: 0.5716\n",
      "Epoch 77/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4424 - acc: 0.5724\n",
      "Epoch 78/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4402 - acc: 0.5731\n",
      "Epoch 79/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4383 - acc: 0.5732\n",
      "Epoch 80/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.4360 - acc: 0.5740\n",
      "Epoch 81/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4336 - acc: 0.5745\n",
      "Epoch 82/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4316 - acc: 0.5750\n",
      "Epoch 83/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.4297 - acc: 0.5758\n",
      "Epoch 84/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4274 - acc: 0.5764\n",
      "Epoch 85/100\n",
      "3402/3402 [==============================] - 36s 11ms/step - loss: 1.4255 - acc: 0.5771\n",
      "Epoch 86/100\n",
      "3402/3402 [==============================] - 36s 11ms/step - loss: 1.4236 - acc: 0.5775\n",
      "Epoch 87/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4214 - acc: 0.5776 0s - loss: 1.4211 - acc:\n",
      "Epoch 88/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4197 - acc: 0.5782\n",
      "Epoch 89/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.4176 - acc: 0.5787\n",
      "Epoch 90/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4160 - acc: 0.5794\n",
      "Epoch 91/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.4140 - acc: 0.5796\n",
      "Epoch 92/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.4120 - acc: 0.5802\n",
      "Epoch 93/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4107 - acc: 0.5806\n",
      "Epoch 94/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4089 - acc: 0.5811\n",
      "Epoch 95/100\n",
      "3402/3402 [==============================] - 36s 11ms/step - loss: 1.4069 - acc: 0.5815 0s - loss: 1.4070 - acc:\n",
      "Epoch 96/100\n",
      "3402/3402 [==============================] - 35s 10ms/step - loss: 1.4055 - acc: 0.5817\n",
      "Epoch 97/100\n",
      "3402/3402 [==============================] - 36s 10ms/step - loss: 1.4038 - acc: 0.5822\n",
      "Epoch 98/100\n",
      "3402/3402 [==============================] - 36s 10ms/step - loss: 1.4022 - acc: 0.5828\n",
      "Epoch 99/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.4006 - acc: 0.5832\n",
      "Epoch 100/100\n",
      "3402/3402 [==============================] - 34s 10ms/step - loss: 1.3990 - acc: 0.5832\n"
     ]
    }
   ],
   "source": [
    "callbacks = [keras.callbacks.TensorBoard(log_dir='p2_log_dir_m3',histogram_freq=1,)]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4),metrics=['acc'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('chgen_model_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 128)               63360     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 35)                4515      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 35)                0         \n",
      "=================================================================\n",
      "Total params: 67,875\n",
      "Trainable params: 67,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import  Activation\n",
    "max_features = 50000\n",
    "maxlen = 100\n",
    "model = Sequential()\n",
    "model.add(GRU(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "4252/4252 [==============================] - 38s 9ms/step - loss: 1.8032 - acc: 0.4706\n",
      "--- Generating with seed: \"vento fainting. i simply wish to hear your real, real opinion.upon what point?in your heart of heart\"\n",
      "------ temperature: 0.2\n",
      "vento fainting. i simply wish to hear your real, real opinion.upon what point?in your heart of heart, which he said her been some way stand that i was a stand and the stare the stare to the stare of the stare to the cortance the stare the stand of the roomself in the sted of the stare the stare the stare the stand of the real the stare to see the stare the stare and all the stare that we have a dearly and was steps the stand of the stare the stare the stare the stare the stare this stand and the\n",
      "------ temperature: 0.5\n",
      "arly and was steps the stand of the stare the stare the stare the stare the stare this stand and the window and see the sting the man which was a steps to his roomself untily sold perhaps. i save anstered to his cristand to see the good stare that i would to care the stare in the tains, the note of looked poor, and i was the and holmes. it had an erder. then the stand of the free, i was all which would po, and it was the terishing, which mare the stant of a roomesty that there is all arather in \n",
      "------ temperature: 1.0\n",
      "would po, and it was the terishing, which mare the stant of a roomesty that there is all arather in the mr. ham reinature.but, a are, nothear,ard, which no detered in anway wanhosw it writed fleared bugg good we lived a cours, and toor inpo armemertion. near thism de glandre,th thay sure horewas forst. becharndigg do my arean toround.''yes, a lardshappiase nor, nich left my managed holles, seriinay, which havrely here, the good thound at the greated morthay nod raous arze i cansceda tany, the al\n",
      "------ temperature: 1.2\n",
      "y, which havrely here, the good thound at the greated morthay nod raous arze i cansceda tany, the alligh to roisting lefbacilyall uttabouthemperhattersolate?''did, sure strang, then, whichthenk amosser opde of, whryours in block, which toled clay an leve, said youmbe so speen wedlyon ditsing phosely.golss. sthere?''you have ceriherhim the objeclia , whed there,and to a if immenfent, i cunninguround, and allasticch.brang-suresmatice nevenidure,was bonder aeverwhen? 'it soroued a derengess' apifig\n",
      "epoch 2\n",
      "4252/4252 [==============================] - 39s 9ms/step - loss: 1.6619 - acc: 0.5098\n",
      "--- Generating with seed: \" had hardly shut the door behind him when holmes rose to put onhis overcoat. there is something in w\"\n",
      "------ temperature: 0.2\n",
      " had hardly shut the door behind him when holmes rose to put onhis overcoat. there is something in which he was the strong to the coll to the coll to the start the coll of the coll of the strong to the coll of the sinter of the start which he was the coll to the coll to the coll the coll might be to the compenstention of the coll start to me to the coll to the start his headd in the coll to the coll to the coll of the coll to the companions, said he, and the man which he had been the coll me to \n",
      "------ temperature: 0.5\n",
      "ll to the coll of the coll to the companions, said he, and the man which he had been the coll me to eask to the come with the companiony would have the coll in the startly here to the stolthe is the sinest sen the companioned that this one when i seem and be better the strictle is in his grome. he wasto come to the companion. the coll of the singled our seems the started simple of the coll of the herroom which were the window? he should say to the man one to the coll of the hands with the toill \n",
      "------ temperature: 1.0\n",
      "herroom which were the window? he should say to the man one to the coll of the hands with the toill in the case. he wrothes with she son the hamged might fron the than there weres on turniem, said he epested better. see,theystilly was not been toboke himself powintor doubtlied, i was came andsul belk is as upon the suining it isemed bettever just bolet to her to clier tomenitiled in quiting with the chail asto. the chan, of thes stact doing his jurtly side shall make the plip of questchandingtha\n",
      "------ temperature: 1.2\n",
      "he chail asto. the chan, of thes stact doing his jurtly side shall make the plip of questchandingthat is been humubiblegraw-roveas si; my chanque, selrestanca pbusssylife hlap oflin he, he dimy.well,well. he werward. it was priboun ganing tht itpurplueally proverence cecle out were tudled a silonly toctagullaned,sin, brut thenmust boilible me down which ourchescempacivechend in tubninit, anced turnered ''this satkned it uponive the atdectosh ronclay-his wations pisce?nothing you air farsovilable\n",
      "epoch 3\n",
      "4252/4252 [==============================] - 40s 9ms/step - loss: 1.6443 - acc: 0.5146\n",
      "--- Generating with seed: \"own apassage, opened a barred door, passed down a winding stair, andbrought us to a whitewashed corr\"\n",
      "------ temperature: 0.2\n",
      "own apassage, opened a barred door, passed down a winding stair, andbrought us to a whitewashed correathall to the clain to me to the could not side of the could not one who was the case of a case of the secrets of the clain of the singular and to see to the could not so and to see to the singular and the could not and a came and to a ground to the country of the clame of the clain to the clamed of the course of the could not and a carriaged to the clain of the clame who have to me to the countr\n",
      "------ temperature: 0.5\n",
      "f the course of the could not and a carriaged to the clain of the clame who have to me to the country and to go tort of the street. i was the own and sweet of mondow of the corried to me to the dressing to me to see you distorted out and to the lattle to feater he was gone at my and to sate and to me to be not methen was and which had to have gone not out to man upon me to the heard of lutter and not been of me, and the can and so was in the complly to me to me to sand to the speated of the secr\n",
      "------ temperature: 1.0\n",
      "not been of me, and the can and so was in the complly to me to me to sand to the speated of the secrited, and slirding in to egtomer,and ir goutn.i should neverf a plapestured my atnoy a inspeat the claie. uploor to are an egqiekeryly doubt ord the claoch upon were ias fricty over a met more ones manyfroyoured some  simped his carriar wric my gold out of evir was swew to, of which had never to simpate to grifbut smallent to the . she was howryly you rede er chink. it  so mis wasfired in two lord\n",
      "------ temperature: 1.2\n",
      "pate to grifbut smallent to the . she was howryly you rede er chink. it  so mis wasfired in two lord, tomites theshouthou he dlpisp thothen dear might befgullyeoias ofh was canrious lane whis dirscid. villed, came to hercus and hil centinling of imported mean-beltumand over.what cardian miss sund, thwand swure opluch krepo ous ophing side uponetre peolly who had not in thagane mabonul, po ntruesk you famuned in a man, ready.the ardl, wence. my eyes weather. lefring nandrow i  not gartto mytencha\n",
      "epoch 4\n",
      "4252/4252 [==============================] - 41s 10ms/step - loss: 1.6443 - acc: 0.5142\n",
      "--- Generating with seed: \"icult it is to bring it home. in this case, however, theyhave established a very serious case agains\"\n",
      "------ temperature: 0.2\n",
      "icult it is to bring it home. in this case, however, theyhave established a very serious case against the wife we was the that i was the the whole that i have a compress which is the adventure and the fact of the to the course that i have a per a street that i was the signs and the as a compress is a compresting to the wife in the street that i have a land which is the street to the to the to the to the press to the to and the police that the course that i was the to the seen to the of the cours\n",
      "------ temperature: 0.5\n",
      "to the press to the to and the police that the course that i was the to the seen to the of the course that i have we have a could see to the to the feirce in the press wife, was the portable who whils was any hous presention and the house of presently which had a pend to the fact of the in the eyes which had early depost a conclus the marrew a dis i advened to eken the fack that i have a secunt minutes of a fellow of the appeared that i have a come of the and either in the street that the street\n",
      "------ temperature: 1.0\n",
      "nutes of a fellow of the appeared that i have a come of the and either in the street that the street. whans, the somewhic the worm. we should gews littlen't clasc was up to frow keek toward his lyffortip and that the ad wamed to traing the digriter, anon who remope the room.how hell. from the twisitech, shishrich but i not an wounds to .than the up that he had a write. this scent and with chans thiryfuted, stillon the plack which we will insemer than you. let og waich which you perepreds. i coul\n",
      "------ temperature: 1.2\n",
      "iryfuted, stillon the plack which we will insemer than you. let og waich which you perepreds. i coulds of recame.thasifreshs letted oh, when he nois ingenely write-engubonlythor, what there allless, uband twis ip acligine thetirbance?''n'a yand a waffer. he is volofflevedry innow you were trees'll rewanded if my laypest clie, reclining to mydels ane off not, he those you wouldpren doing from see ashood he to?'youswouldlberclient ?well,but that you know thirds turn?no have foot alant put. it who \n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.01),metrics=['acc'])\n",
    "\n",
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(X, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, chars_to_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "4252/4252 [==============================] - 39s 9ms/step - loss: 1.8076 - acc: 0.4687\n",
      "--- Generating with seed: \"aysglided away to some other topic, until at last i gave it over indespair. it was not yet three whe\"\n",
      "------ temperature: 0.2\n",
      "aysglided away to some other topic, until at last i gave it over indespair. it was not yet three when i am she would be and gottered the strange in the seceinger and were she was a little with a straight of the strong in the state and the man which is the strange of the double that it was a street and the street me to the mean of the strange in the countrance to a strange in the man which is the days in the coundred the strange in the stand in the state and the strong him when i was all the coun\n",
      "------ temperature: 0.5\n",
      "ays in the coundred the strange in the stand in the state and the strong him when i was all the coundroubled a start me but that we have somestrange man, and the since and the good in the poost but the strong a rough for i to a bring as we had the bed and is were a chice of the trueched a man with a must be the man, the cirel. i am speak that it was the cinest, of course to the lood and was to see which the bed, and the fath of the fable, and the was one a repare of the man of in the simple and \n",
      "------ temperature: 1.0\n",
      " which the bed, and the fath of the fable, and the was one a repare of the man of in the simple and see that i shand blicoussturther gue with his law you, what ourred- away that i sa tertice. i douittles then was sunge-so otnive got not as much which i as povesiof of indind mysell. what speak doubly with at the carisy herakening from fingy my sher good my outlecare that he ware the son moles.for whatial that you willorty on the shexime. the cire in own through our lough dishirm,therewerefaffieft\n",
      "------ temperature: 1.2\n",
      "hatial that you willorty on the shexime. the cire in own through our lough dishirm,therewerefaffieftclostle wawh meying of suchventible whicning doust cussultd, soat  as other wianrelwo seprest a muther -not, thatyes oveding us belloot.afverilitual orpecced, any your righfo eaul itranioned the musincy mes, i was us take in trrucker splice. his namilesbutity the pepcecry cwost,are when i mevingtoteming .and ibouthuns, truetect it a lijfrfore, pix calroines,and the faasyo rust unsmust sumbly 'par \n",
      "epoch 2\n",
      "4252/4252 [==============================] - 40s 9ms/step - loss: 1.6642 - acc: 0.5093\n",
      "--- Generating with seed: \"ifts ofthe clouds. holmes drove in silence, with his head sunk upon hisbreast, and the air of a man \"\n",
      "------ temperature: 0.2\n",
      "ifts ofthe clouds. holmes drove in silence, with his head sunk upon hisbreast, and the air of a man was a consece of the counted to the possible to the beate to the letter to the consece of the counted to the counted to the course in the counted to the counted to the coronered to the course in the counted to the consece of the counted to the chair have been and the consent of the consece of the door in the room, and the counter to the counted to the counted to the counted to a countle of the cou\n",
      "------ temperature: 0.5\n",
      "or in the room, and the counter to the counted to the counted to the counted to a countle of the countrove the counted his langagher of the come and was and there asked asleave been the counted of the consece in the charar is no ment was not into the sight to meder to the eation in the distor of the counted a main do bohe in the chare in the room, and i shall be done a greet door, i was all to the more should could cause, and ready country in the carreaded me to a waitter. i got have a small to \n",
      "------ temperature: 1.0\n",
      " more should could cause, and ready country in the carreaded me to a waitter. i got have a small to take. your surpeeped she letterther with himout could passought her to there a poley hthere is our mationed in hammerlead very amerided, unst ivis. i do dother shoildice,  juad mope, i was ownfirdoge theresamen me wishet not offolowleagins. so comeogethe lew took so take let, so be enougity of the lifter when hers was very noorsides.three's chire caure, but my eyes.i was as the sinples, and she se\n",
      "------ temperature: 1.2\n",
      "ifter when hers was very noorsides.three's chire caure, but my eyes.i was as the sinples, and she seemed along time all dear, oitherelang itairnallybat of the no meads, rylear that.i cruster histlort frol possifion.you wetter. ihavementalouplesfroad me pusioes and rountyloss. i worrod woubtled into trew him?i havije, asker, would gh. has servotce. d'stone viledtoy getal voice, if wwito muhte meal by bired backer to-afaced hisder-bage. shill hakspuge amginnder.s  that his tate hatprese to theing \n",
      "epoch 3\n",
      "4252/4252 [==============================] - 41s 10ms/step - loss: 1.6481 - acc: 0.5130\n",
      "--- Generating with seed: \"the way of his little game, like those out-and-outpirates who will leave no survivor from a captured\"\n",
      "------ temperature: 0.2\n",
      "the way of his little game, like those out-and-outpirates who will leave no survivor from a captured that i could not see the start which was a said the strains of the straight of the case of the dear that i could not see the dead that i could not have been the considerary of the companion which was on the companion to the case of the considerary one of the companion that i could not see that he shoute of the son which was a started and the death which i had seen of the considerary straiged of t\n",
      "------ temperature: 0.5\n",
      "houte of the son which was a started and the death which i had seen of the considerary straiged of the day which were cate serloon that i start him in which were an leen ear in at the conditia assions which was a from i can the strong part were upon the chartionared to his the companion which was a stand which was a nown said holmes, said holmes, in the strange of the came such with a case of the longglard of the for one of the sewn that i could not corner conceraince of the cause in the matter \n",
      "------ temperature: 1.0\n",
      "longglard of the for one of the sewn that i could not corner conceraince of the cause in the matter of the tuarance which whought reacce. and lutter. there will call that she suages that i course the states such the table against a little onles we mbjest from the motage of started in ent round without and down in -he will in the hunday sree what grew siek his. his bo.nas--staw on her clooks answer, i will and grokninaly seacurure. the  accorner day adrepts. you. i conculatured nexin of ashed ope\n",
      "------ temperature: 1.2\n",
      ", i will and grokninaly seacurure. the  accorner day adrepts. you. i conculatured nexin of ashed openso't.garven inharturethrown-hert.you crangeler,boardy s. lasfeil thises of garmside what long.his eveknight the shouaquotour wind ris to see with my dewurd deser. thoney jurts onethoughtself to werry else which tires to nerrip, and had chadrably, suff, buthing from you could vernt essisn thatthe son knever.agsg the larme up.no, but the noth, has.i was ofthis lainingas his imance. was or subgeded \n",
      "epoch 4\n",
      "4252/4252 [==============================] - 40s 9ms/step - loss: 1.6472 - acc: 0.5125\n",
      "--- Generating with seed: \", remarked the strange gentleman,we've had just a little too much secrecy over this businessalready.\"\n",
      "------ temperature: 0.2\n",
      ", remarked the strange gentleman,we've had just a little too much secrecy over this businessalready. the hands the startion the course of the little made the possible which is the startion. the first the strange and the stall the possible the startally the possible the stalle all the the strange and the startion the course the stall the most the startion. the strung in the considerable the countral the side was all the sight had been the pooned the startion the startion. the startion, and the st\n",
      "------ temperature: 0.5\n",
      "l the side was all the sight had been the pooned the startion the startion. the startion, and the strung in the fawn the boad the could nothed the party, and the rushed the the country of the the possibl such a perportance at the lady the other the under the portance which should he endlands the corruce have been the certay. but the head that they is the simple the case that the case the man the start the other contract my restive and the little stout that the able. the upon the shage the days t\n",
      "------ temperature: 1.0\n",
      "tart the other contract my restive and the little stout that the able. the upon the shage the days that which shall her klowly yeirseever fart, nother days thing up out correed thefant, he so know would sew taking them hemithing. the boods, of streeted that the therelive that the nase of somet in the sire upon harfrill be to be i had bewing ploud unces the wife likened upon you ladsturson. dy; but i git that pepered and was sameroand stood my huntity. i know. the kentmaually in be andive allof, \n",
      "------ temperature: 1.2\n",
      " i git that pepered and was sameroand stood my huntity. i know. the kentmaually in be andive allof, these he thudveyy back the xoothed the awyess.the was all be hit of 'whi un a onlyed that the such brisy moutlems wayed my wellar. but mr. holmes, were its thesaysould mythena vious paperhers thethis mostep with,that factful which senit mieyowhorsuch my. whom ours,that he a pound so the driwd and now thatn, dea, ur .they sarves in it pisevous.would last, of inlled, who wordnd, as thatcoeebblinally\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.01),metrics=['acc'])\n",
    "\n",
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(X, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, chars_to_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp 0.2\n",
      "last night police-constable cook, of the hdivision, on duty near waterloo bridge, heard a cry for her the police which had the stalle the person the course the hands. the start the startion and the stall the possible a portance and the possible the startion. i shall be the startion to the course the pooned the best the startions. the had the the strange and the possible the possible the possible which heard the the hands the startion. the has the struck the strange able the would be the have the startion. the strange the case of the more the about the the startions. the startion that the stand\n",
      "Temp 0.5\n",
      "iage. wasunder-secretary for the colonies in a late administration. theduke, his father, was at one tracked the serthe bring minuted of the able of the the face the salked the the able the marrowed of the stably say the time the startice all the suddenling had her brown the litter were probublir. the sumpley had been whom the person the wood the that the able the hours said helded the struck the pered the appleach, my the boller a case the faction of the the street. i think that it was the course the case alitter all the possibl three the formed the simple case at the strunger of the hours sam\n",
      "Temp 1.0\n",
      "house festivities, is an only child,and it is currently reported that her dowry will run toconsideralsed this acclike, forirs in there.i liknes. moredose usen, have bedowinaling. i should have see it.thereyew and shourderst reasons. the stoudally his, and the custre., asking the came ear bell slowed the reory, with thepechagion. with that the corrobe abdow, it is more my fe't in the splarhing with mr. is could you besincall  that is lye, a morefordised afrance, were on. 'your case.thewere and unfl, i came pephowed as that he hurd oved us.there should seener. it was tarmed them pone. i worhmy b\n",
      "Temp 1.5\n",
      "rner up to the dressing-room of the countessof morcar upon the day of the robbery in order that he mustlemanrging all. siling-huvanetslide that s pay ther'., gave puler, sodhumapplox and tclops.yesild im!ouf a secumel-by turning fullmisshouxe fanmiss hmrasponghot so takle ran--you think,har-mysersulusllomed our tound gave des ffidisop, wereddearsh ofpress., as dostuarresos afticm of you semras offe?-by as?no,.tho? myselmgnirotherpepairyatruamarpomelife their i'llas?i' came assi mystrunchmscorrylerdded a pleacnestance. thatsupround yfer! shap sire, turned, toatectsedghim firs eaf.what ekild the\n",
      "Temp 2.0\n",
      "tains upon his rightshirt-sleeve, but he pointed to his ring-finger, which had beencut near the naildetdale ayenour ghop.queatail,a peceojuphowwo'd,fact it  asfielightquie all mon's,ly tofustuamallicinctioor. at laveraimp.foelverleviroriusqyalk.othes's.my lasci h?bjgif  whi.her.thil!., thum ipassnnasparsa extrl blow,.' lrivefouwihe.ciessie, eumous!ve yqualre while h'p loepomedopoursabsma pittutin wppart; hun's!t keye,whisk, we saisdern,not viecist!'ash is.awrryswibl-'hon'larbd mom, fals?'whersen omal!hegf-iptor's,two is,' shoater taadn iecipen!sir.mpuf! take?nlye'splaitilg s.thanded nesras occ\n"
     ]
    }
   ],
   "source": [
    "print(\"Temp 0.2\")\n",
    "print(generate_text(500, 0.2))\n",
    "print(\"Temp 0.5\")\n",
    "print(generate_text(500, 0.5))\n",
    "print(\"Temp 1.0\")\n",
    "print(generate_text(500, 1.0))\n",
    "print(\"Temp 1.5\")\n",
    "print(generate_text(500, 1.5))\n",
    "print(\"Temp 2.0\")\n",
    "print(generate_text(500, 2.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length, diversity):\n",
    "    # Get random starting text\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    for i in range(length):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, chars_to_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_to_chars[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "#print(generate_text(500, 0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 128)               83968     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 35)                4515      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 35)                0         \n",
      "=================================================================\n",
      "Total params: 88,483\n",
      "Trainable params: 88,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import  Activation\n",
    "max_features = 50000\n",
    "maxlen = 100\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_len, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "4252/4252 [==============================] - 42s 10ms/step - loss: 1.7778 - acc: 0.4775\n",
      "--- Generating with seed: \"you wish?when mrs. turner has brought in the tray i will make it clear toyou. now, he said as he tur\"\n",
      "------ temperature: 0.2\n",
      "you wish?when mrs. turner has brought in the tray i will make it clear toyou. now, he said as he turned the states and the states of the states of the states of the states and the states of the states in the states of the been and the considention of the companion of the companion of the parting of the states of the state of the states of the light to the companion of the police of the street of the states of the pired of the companion of the states of the state and the street of the singular th\n",
      "------ temperature: 0.5\n",
      "he states of the pired of the companion of the states of the state and the street of the singular that he are rucastle train in the wine of the way for the graving a stepped the hall in the tome backand the side of his states and the shoted with the streets of the state and are in the comes of the one of the dray of the worked with his house in me, interested to the very in a banker one, the dear of his morning of the comessing of the states of the crives was and the passed of the trous of him a\n",
      "------ temperature: 1.0\n",
      "of his morning of the comessing of the states of the crives was and the passed of the trous of him and inthe avouring up and door father was young pilled there we could not youwas in me?this dather among all thimghessen, which we had allaic absolute. i could be which in a dake throlise asceitbiturationawande all to one bsend dearing-holmessured in the blacked backs which con lounder to whech think mr. holmes this stind, and we am the! it was to be one that i taken the very name tist a goody of s\n",
      "------ temperature: 1.2\n",
      " mr. holmes this stind, and we am the! it was to be one that i taken the very name tist a goody of scrgel over for ivan; by unfourbles doable amone my could crose, shatly gad notoaty. ampertiia you should come hisflad. t ihamfere bitwho am, incrotm by facts leathand deadly did he work?iesverap no s any one of astank,when you stattenged.you ruidain twether alte what runcother me togrwat, so. rupoler it with is as it bhistle mcretoningucfessed that the. lip, we endey, and piddacen what has comagh \n",
      "epoch 2\n",
      "4252/4252 [==============================] - 42s 10ms/step - loss: 1.5369 - acc: 0.5457\n",
      "--- Generating with seed: \"ng-rod had shrunk so as not quite tofill the socket along which it worked. this was clearly the caus\"\n",
      "------ temperature: 0.2\n",
      "ng-rod had shrunk so as not quite tofill the socket along which it worked. this was clearly the cause of the colour, and i have been make the country from the matter of the start of the start of the colour. i have a long of the street of the country of the doors of the word of the room. i have a such of the country of the lady of the street of the country of the street of the country of the country of the matter of the word of the street of the street of the stain of the country which i have bee\n",
      "------ temperature: 0.5\n",
      "y of the matter of the word of the street of the street of the stain of the country which i have been the day of the perhaps should be sure a window will have not you to company, in the simple of the sumpray in a table. i thought i have been of the cullent of the office at the other of the door clear of the fore was so me the door, but i can on the servest of the street, when i was present the signer of the count of the summonate and heard the chair. it is a great will be some matter dead the ma\n",
      "------ temperature: 1.0\n",
      "ner of the count of the summonate and heard the chair. it is a great will be some matter dead the matter which was aprosk upon black in the lame pracariace obmingto the colour of companiod groundthing opening hishead someshe alsolow. so better streal those of all me perhead home st. clopping had me.cainch that the maser here, said he-turned. i refect be sion, which to it allown in his ire and particulose son and still whilver, what to be largefell us formed, of himpaphels. yas follow here, sense\n",
      "------ temperature: 1.2\n",
      "iculose son and still whilver, what to be largefell us formed, of himpaphels. yas follow here, sensefocking un.i was muchus?ve answered every felled bedrake, alone first, and i am follotixented vullam and have you ove., ah.and.had pave ascruckteted nigh well, un was veorebil, where ass. hownihid my ey: turn thosering, wals presentered nimpen young tum the are notckleaged of thenace our tabll, i de.thingeas in his.' of the droune ayjew had holmes.eek', and   ired. gual open behind, but amno the p\n",
      "epoch 3\n",
      "4252/4252 [==============================] - 41s 10ms/step - loss: 1.4843 - acc: 0.5600\n",
      "--- Generating with seed: \"ngle advertisement. every shade of colour theywere--straw, lemon, orange, brick, irish-setter, liver\"\n",
      "------ temperature: 0.2\n",
      "ngle advertisement. every shade of colour theywere--straw, lemon, orange, brick, irish-setter, liver, and i had a some street which i had been a should be a should be a pare and the papers and a sister of the confided of the sister of the silence of the stranger was and the matter was a consideration of the side of the street of the contrast of the street of the consideration of the simple of the consideration of the consideration of the contrast of the secret of the street of the little which h\n",
      "------ temperature: 0.5\n",
      "consideration of the consideration of the contrast of the secret of the street of the little which hour before it was leather have considered his points of a pare and the goose and a gentres, and the bird which made it should be a consideration which you a shoulder by the for a chamber and came to the door.you are out of the attempt of him which i was some singular very find a more and stood beauther an investight of something was anyone will interesting, mr. hos-s of my points of the other coul\n",
      "------ temperature: 1.0\n",
      "her an investight of something was anyone will interesting, mr. hos-s of my points of the other could not  horropud, we as so breashed you, boan.ug theusnotume was discondularu-cry in action, i had cleared togetherd over the door lift, but turn it of frest. theexiently it letters in my. i came for the compatien glame compativer. i tall into a side surpoperntoned and a pocurative of firstother anispending.' no sire.s i in sabude that it-ould see what first had all, again up a shellul the silarran\n",
      "------ temperature: 1.2\n",
      "pending.' no sire.s i in sabude that it-ould see what first had all, again up a shellul the silarrand. hewas madyof pace, but at finds olotic upon the 'other.evennot found there leftmoren beennwelph upon it.my in?you, theneredinity trever eastaque diufusts houry being your windowhat well w his fe-tropashed never boin answering in a flloussum minute will sir bridge, answhickled. as weredubcted i. he tast two, crumper to lestrecibed from my matter about fromunateof me and moutnestgetiviched toches\n",
      "epoch 4\n",
      "4252/4252 [==============================] - 41s 10ms/step - loss: 1.4564 - acc: 0.5678\n",
      "--- Generating with seed: \"go if a real effect were to be produced. therewere meetings, and an engagement, which would finally \"\n",
      "------ temperature: 0.2\n",
      "go if a real effect were to be produced. therewere meetings, and an engagement, which would finally the same satisfections of the street. i have a man and the country and which has been and one of the same way of the same of the stairs of the sight of the same of the same man who had been the street, and the stairs of the same passion of the sign of the same of the door. i should be a light that i have seen many with an excellent and started and all the same the country of the street. i should b\n",
      "------ temperature: 0.5\n",
      " have seen many with an excellent and started and all the same the country of the street. i should be a property of the saecher. he was a short great face which should not see the passional was and man that i should be every stepped for his hatting with the passage which i have been for his sherlock holmes one of the shoulder of a maid in the few journest shoulder been the while on the sign of so which has interest and strongene of my face of the the time of the bedroom of the england of my stat\n",
      "------ temperature: 1.0\n",
      "which has interest and strongene of my face of the the time of the bedroom of the england of my states. with might. i may ingars bleak upon the it out of a city of sill paper, at his polictost. itwas 'brightious pation of any shoulding pade bried to therewas a scatter-strikenor jobands sation.had breaking for holf booning in his breakfar out, and the spancity. it iseven sbeembling in the partotragced ultold at the afied, but imagiful, for for this lady slight lise pister, and she present it engi\n",
      "------ temperature: 1.2\n",
      "ced ultold at the afied, but imagiful, for for this lady slight lise pister, and she present it engineed. well,y.it hadlavernoutisaverbak ah and examined to his spete of least tads. there was  if the res. passs that madybright own.wore shutterhereur, noatroy?excesier from opens handhearing to follomarder up, glancing in the topgosidea any furney which was breakbuiled eighbsond yor his plades wruttered out is theexaitive t'tinh.this feary.this of here g, with everbediance, 'mr. gethedorclageriusm\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.01),metrics=['acc'])\n",
    "\n",
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(X, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, chars_to_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doubt, caught the clink of our horse's feet.but why are you not conducting the case from baker street. it was a small of the chair of the sign of the same passion of a shall in the coronet of the matter was a state and and a short of the country of a companion of the chair which i had been the same and the door of the same of his hands of the same of the same close of the sign of the close of the same back of the street. i have a sharped to the same and for a stains of the matter and a shoulders. it was a sonder was and started to the passion of the matter of the word which i have seemed to th\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(500, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 â€“ Use the Best Model to make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('chgen_model_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the user input\n",
    "#text_input = np.array([input()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_chars(model,input_chars,num_to_predict):     \n",
    "    # create output\n",
    "    predicted_chars = ''\n",
    "    for i in range(num_to_predict):\n",
    "        # convert this round's predicted characters to numerical input    \n",
    "        x_test = np.zeros((1, window_size, len(chars)))\n",
    "        for t, char in enumerate(input_chars):\n",
    "            x_test[0, t, chars_to_indices[char]] = 1.\n",
    "\n",
    "        # make this round's prediction\n",
    "        test_predict = model.predict(x_test,verbose = 0)[0]\n",
    "\n",
    "        # translate numerical prediction back to characters\n",
    "        r = np.argmax(test_predict)                           \n",
    "        #r = np.random.choice(len(test_predict), p=test_predict)                           # predict class of each test input\n",
    "        d = indices_to_chars[r] \n",
    "\n",
    "        # update predicted_chars and input\n",
    "        predicted_chars+=d\n",
    "        input_chars+=d\n",
    "        input_chars = input_chars[1:]\n",
    "    return predicted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "the adventures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-h\"\n",
      "\n",
      "predicted chars = \n",
      "eaded and should be a companion. i have no doubt that i have no doubt that i have no doubt that i have no doubt that i have not the companion of the street and since in the street and street and sently and such a lady street. i had not been such a little problem. i should be some companion. i shall \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "blue carbuncleviii. the adventure of the speckled band  ix. the adventure of the engineer's thumb   \"\n",
      "\n",
      "predicted chars = \n",
      "family street. i should be some companion. i shall not have been some of the secure of the street and sently and such a lady street. i had not been such a little problem. i should be some companion. i shall not have been some of the street and street. i should be some companion. i shall not have bee\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "er passions, save with a gibe and a sneer. theywere admirable things for the observer--excellent for\"\n",
      "\n",
      "predicted chars = \n",
      " the street and singular the street and street and sently and such a large and such a large and such a large and such a large and should be a companion. i have no doubt that i have no doubt that i have no doubt that i have no doubt that i have not the companion of the street and since in the street \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "oks, and alternating fromweek to week between cocaine and ambition, the drowsiness of thedrug, and t\"\n",
      "\n",
      "predicted chars = \n",
      "he street and street and sently and should be a companion of the street and street and sently and such a lady street. i had not been such a little problem. i should be some companion. i shall not have been some of the street and street. i should be some companion. i shall not have been some companio\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "a gasogene in the corner. then hestood before the fire and looked me over in his singularintrospecti\"\n",
      "\n",
      "predicted chars = \n",
      "on of the secure of the street and street and singular the street and street and sently and such a lady street and street and sently and such a lady street. i had not been such a little problem. i should be some companion. i shall not have been some of the street and street. i should be some compani\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_inds = [0, 256, 1024, 2048, 4096]\n",
    "\n",
    "# load in weights\n",
    "max_features = 50000\n",
    "max_len = 100\n",
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape=(window_size, len(chars))))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.load_weights('chgen_model_3.h5')\n",
    "\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(model,input_chars,num_to_predict = 300)\n",
    "\n",
    "    # print out input characters\n",
    "    print('------------------')\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "the adventures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-h\"\n",
      "\n",
      "predicted chars = \n",
      "eaded which apondown the whole or elighednsown buring would saw companyment mand.firshatoes! i opeded.how come, i canding her dury, deduly come. the tack thefrom me, so down thing, and one cabry of the lady.''and john. why may is some fing brought besilation.''or have not murder, cat indeed had been\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "blue carbuncleviii. the adventure of the speckled band  ix. the adventure of the engineer's thumb   \"\n",
      "\n",
      "predicted chars = \n",
      "timster laids. i had followed me know ereeved,''but then, yis done your narress.did your enfinally cannot featurence.''when i say upon her. yes.were mry trave yeardy as hardleakeal, lounged and laoding manning our introm cable to stay oirs.and threw will posites,depurside theestable gince, but it is\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "er passions, save with a gibe and a sneer. theywere admirable things for the observer--excellent for\"\n",
      "\n",
      "predicted chars = \n",
      "ely ofms. my was place offener within a racallatter stepfation-slood to do what thesen's two vinibsting of engly having presence of a duside in the first senes ink, andhorrisgable. you canes, he half--at.my dear holmeam. i thy round the momessast exappients. i sntrept to be notnown grion of leaving \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "oks, and alternating fromweek to week between cocaine and ambition, the drowsiness of thedrug, and t\"\n",
      "\n",
      "predicted chars = \n",
      "o me, nettering our midles pitchings reg. it bestonly outinstantly seek walk, lefn-sponder to gurtof lets over?it was unforthing an all. see i shouldn-brase. there are nothing criety. nowour demvenagemy upand, and we had come, you heard up a holdene which i had most offiched of the mater usmisfactio\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "a gasogene in the corner. then hestood before the fire and looked me over in his singularintrospecti\"\n",
      "\n",
      "predicted chars = \n",
      "on of his eveningle up to the coose?for it could without this little clear, and where the purpare, and as a sut basage, which since, ofsering that the clime from problem. done his faceely and mattering reclicient your indeaus of my chamiswed in the speel wandaving,  hutwrontly he would bott and untr\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_inds = [0, 256, 1024, 2048, 4096]\n",
    "\n",
    "# load in weights\n",
    "max_features = 50000\n",
    "max_len = 100\n",
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape=(window_size, len(chars))))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.load_weights('chgen_model_3.h5')\n",
    "\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(model,input_chars,num_to_predict = 300)\n",
    "\n",
    "    # print out input characters\n",
    "    print('------------------')\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "antage, they may do ussome harm unless we are careful. i shall stand behind this crate,and do you co\"\n",
      "\n",
      "predicted chars = \n",
      "me to me that i have no doubt that i have no doubt that i have not the companion to me that i have no doubt that i have no doubt that i have no doubt that i have not the companion of the street and since in the street and street and sently and such a lady street. i had not been such a little problem\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "increased by the two guardsmen, whotook sides with one of the loungers, and by the scissors-grinder,\"\n",
      "\n",
      "predicted chars = \n",
      " and the street and street and sently and such a lady street. i had not been such a little passion of the street and street. i should be some companion. i shall not have been some of the secure of the street and sently and such a lady street. i had not been such a little problem. i should be some co\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "omething depressing andsubduing in the sudden gloom, and in the cold dank air of thevault.they have \"\n",
      "\n",
      "predicted chars = \n",
      "been some of the street and since in the secure of the street and sently and such a little problem. i should be some of the street and since in the street and street and sently and such a lady street. i should be some of the street and street. i shall not have been some of the street and sently the \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "ked into the room.accustomed as i was to my friend's amazing powers in the use ofdisguises, i had to\"\n",
      "\n",
      "predicted chars = \n",
      " be a companion. i shall not have been some of the street and sently and such a little problem. i should be some companion. i shall not have been some of the street and street. i should be some companion. i shall not have been some companion. i shall not have a little problem. i should be some compa\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "tor and two officers waiting at the front door.then we have stopped all the holes. and now we must b\"\n",
      "\n",
      "predicted chars = \n",
      "e a singular that i have no doubt that i have no doubt that i have no doubt that it is a companion. i shall not have been some of the street and since in the street and street. i should be some companion. i shall not have been some of the street and since in the street and since in the street and st\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "start_inds = [random.randint(0, 99850) for _ in range(5)]\n",
    "\n",
    "# load in weights\n",
    "max_features = 50000\n",
    "max_len = 100\n",
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape=(window_size, len(chars))))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.load_weights('chgen_model_3.h5')\n",
    "\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(model,input_chars,num_to_predict = 300)\n",
    "\n",
    "    # print out input characters\n",
    "    print('------------------')\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "the adventures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-h\"\n",
      "\n",
      "predicted chars = \n",
      "eaded before the black street. i have been a son and the words of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "blue carbuncleviii. the adventure of the speckled band  ix. the adventure of the engineer's thumb   \"\n",
      "\n",
      "predicted chars = \n",
      "bsenve that i have been a corner of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "er passions, save with a gibe and a sneer. theywere admirable things for the observer--excellent for\"\n",
      "\n",
      "predicted chars = \n",
      " the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the s\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "oks, and alternating fromweek to week between cocaine and ambition, the drowsiness of thedrug, and t\"\n",
      "\n",
      "predicted chars = \n",
      "he man who had been a corner of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to t\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "a gasogene in the corner. then hestood before the fire and looked me over in his singularintrospecti\"\n",
      "\n",
      "predicted chars = \n",
      "on of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_inds = [0, 256, 1024, 2048, 4096]\n",
    "\n",
    "# load in weights\n",
    "max_features = 50000\n",
    "max_len = 100\n",
    "model = Sequential()\n",
    "model.add(GRU(128,input_shape=(window_size, len(chars))))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.load_weights('chgen_model_2.h5')\n",
    "\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(model,input_chars,num_to_predict = 300)\n",
    "\n",
    "    # print out input characters\n",
    "    print('------------------')\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "rations havegone so far that we cannot risk the presence of a light. and,first of all, we must choos\"\n",
      "\n",
      "predicted chars = \n",
      "e the matter of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      " the deuce that could havebeen.iii.i slept at baker street that night, and we were engaged upon ourt\"\n",
      "\n",
      "predicted chars = \n",
      "hing of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states o\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      " or anything butreal bright, blazing, fiery red. now, if you cared to apply, mr.wilson, you would ju\"\n",
      "\n",
      "predicted chars = \n",
      "st as a corner of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of th\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "ell mewhat had become of the red-headed league. he said that he hadnever heard of any such body. the\"\n",
      "\n",
      "predicted chars = \n",
      "re is a corner of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of th\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "the mask from his face and hurled it upon the ground. youare right, he cried; i am the king. why sho\"\n",
      "\n",
      "predicted chars = \n",
      "uld have been so for the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of the states of the startled to me to the state of\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "start_inds = [random.randint(0, 99850) for _ in range(5)]\n",
    "\n",
    "# load in weights\n",
    "max_features = 50000\n",
    "max_len = 100\n",
    "model = Sequential()\n",
    "model.add(GRU(128,input_shape=(window_size, len(chars))))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.load_weights('chgen_model_2.h5')\n",
    "\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(model,input_chars,num_to_predict = 300)\n",
    "\n",
    "    # print out input characters\n",
    "    print('------------------')\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "the adventures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-h\"\n",
      "\n",
      "predicted chars = \n",
      "ead the staint of the staining and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "blue carbuncleviii. the adventure of the speckled band  ix. the adventure of the engineer's thumb   \"\n",
      "\n",
      "predicted chars = \n",
      "the street of the staining and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "er passions, save with a gibe and a sneer. theywere admirable things for the observer--excellent for\"\n",
      "\n",
      "predicted chars = \n",
      " the started to the started and the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of th\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "oks, and alternating fromweek to week between cocaine and ambition, the drowsiness of thedrug, and t\"\n",
      "\n",
      "predicted chars = \n",
      "he stall to the staint of the staining and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the s\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "a gasogene in the corner. then hestood before the fire and looked me over in his singularintrospecti\"\n",
      "\n",
      "predicted chars = \n",
      "ng the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stai\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_inds = [0, 256, 1024, 2048, 4096]\n",
    "\n",
    "max_features = 50000\n",
    "max_len = 100\n",
    "model = Sequential()\n",
    "#model.add(Embedding(max_features,35,input_length=max_len))\n",
    "model.add(SimpleRNN(256, input_shape=(window_size, len(chars))))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.load_weights('chgen_model_1.h5')\n",
    "\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(model,input_chars,num_to_predict = 300)\n",
    "\n",
    "    # print out input characters\n",
    "    print('------------------')\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "he, 'his name was william morris. he was a solicitorand was using my room as a temporary convenience\"\n",
      "\n",
      "predicted chars = \n",
      " of the started and the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the stall of the stain the \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "ppiness, gently waving his long, thin fingers in time to themusic, while his gently smiling face and\"\n",
      "\n",
      "predicted chars = \n",
      " the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the s\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "oblem. when you drove home after the concert icalled upon scotland yard and upon the chairman of the\"\n",
      "\n",
      "predicted chars = \n",
      " staining and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the st\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "putting myself in his power. ihave come incognito from prague for the purpose of consultingyou.then,\"\n",
      "\n",
      "predicted chars = \n",
      " and the street of the staining and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall an\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "m the lowerpart of the face he appeared to be a man of strong character,with a thick, hanging lip, a\"\n",
      "\n",
      "predicted chars = \n",
      "nd the street of the staining and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and the stall and \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "start_inds = [random.randint(0, 99850) for _ in range(5)]\n",
    "\n",
    "max_features = 50000\n",
    "max_len = 100\n",
    "model = Sequential()\n",
    "#model.add(Embedding(max_features,35,input_length=max_len))\n",
    "model.add(SimpleRNN(256, input_shape=(window_size, len(chars))))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.load_weights('chgen_model_1.h5')\n",
    "\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(model,input_chars,num_to_predict = 300)\n",
    "\n",
    "    # print out input characters\n",
    "    print('------------------')\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "the adventures of sherlock holmes by sir arthur conan doyle   i. a scandal in bohemia  ii. the red-h\"\n",
      "\n",
      "predicted chars = \n",
      "eaded to her been and the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the c\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "blue carbuncleviii. the adventure of the speckled band  ix. the adventure of the engineer's thumb   \"\n",
      "\n",
      "predicted chars = \n",
      "the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of t\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "er passions, save with a gibe and a sneer. theywere admirable things for the observer--excellent for\"\n",
      "\n",
      "predicted chars = \n",
      " the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "oks, and alternating fromweek to week between cocaine and ambition, the drowsiness of thedrug, and t\"\n",
      "\n",
      "predicted chars = \n",
      "he count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of th\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "a gasogene in the corner. then hestood before the fire and looked me over in his singularintrospecti\"\n",
      "\n",
      "predicted chars = \n",
      "on of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the count of the coun\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_inds = [0, 256, 1024, 2048, 4096]\n",
    "\n",
    "max_features = 50000\n",
    "max_len = 100\n",
    "model = Sequential()\n",
    "#model.add(Embedding(max_features,35,input_length=max_len))\n",
    "model.add(SimpleRNN(256, input_shape=(window_size, len(chars))))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.load_weights('chgen_model_1_test.h5')\n",
    "\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(model,input_chars,num_to_predict = 300)\n",
    "\n",
    "    # print out input characters\n",
    "    print('------------------')\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
